<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Quixotic Quotes</title> <link rel="icon" href="qq.jpeg">
    <link href="style.css" rel="stylesheet" type="text/css" />
    <script src="https://d3js.org/d3.v4.min.js"></script>
    

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="http://d3js.org/d3-selection-multi.v1.js"></script>
    
</head>

<body>
    <p id="title">Quixotic Quotes</p>
    <div id="tabs">
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a class="active" href="project_description.html">Project Description</a></li>
            <li><a href="quotegenerator.html">Quote Generator</a></li> 
            <li><a href="twitter.html">Twitter Bot</a></li>
            <li><a href="vis.html">Visualizations</a></li>
            <li><a href="aboutus.html">About Us</a></li> 
        </ul>
    </div>
		
	<div class="summary">
	<h3>Why Quotes?</h3>
		<p>We decided for our project to generate quotes.  
		The quotes we produce are short and sweet, and are hopefully optimistic or funny.  
		We are posting our products on twitter, with the hope that reading a positive quote can also positively influence a person’s day or attitude.</p>
	</div>
	
	<div class="summary">
    <h3>Data Set</h3>
	   <p>The original dataset used to train the model is from Famous Quotes Database: <a href="https://thewebminer.com/buy-famous-quotes-database">https://thewebminer.com/buy-famous-quotes-database</a>
	   <br>This data consists of 75,966 rows and three columns, the Quote, Author and Genre.  The table below shows a sample of the data:</p>
		 <br/><img src="datatable.png" alt="Data Table Example" style="width:50%; display: block; margin-left: auto; margin-right: auto;">
	</div>
	
	<div class="summary">
    <h3>Data Cleaning</h3>
	   <p>For this dataset, the data cleaning process was relatively simple.
	      The Quotes, Authors, and Genres were all formatted correctly and there were no null-objects.  
		  We had to remove 14,895 quotes, but we were still left with 61,071 unique quotes to use for our model.  
		  We also added a column that checks to see if the quote is less than or equal to 280 characters, which is the maximum length a tweet can be, number of words in a quote and number of characters in a quote.  
	      An example of what the data looks like can be seen in the table below:</p>
		<br/><img src="datatable2.png" alt="Data Table 2 Example" style="width:50%; display: block; margin-left: auto; margin-right: auto;">
		<p>This data is used to explore the statistical properties via visualizations which can be seen by clicking on the <strong>Visualizations</strong> tab above.</p>
	</div>
	
	<div class="summary">
    <h3>Twitter Data</h3>
	   <p>To be able to continuously update our data and algorithm that produces our quotes we needed a way to obtain new data. 
		   We achieved this by using twitter accounts. After searching for the most inspiring and well-structured 
		   twitter accounts that post often we selected four accounts to pull data from. The accounts we chose are: </p>
		   <br/><img src="twitter_accounts.png" alt="Twitter Acounts" style="width:50%; display: block; margin-left: auto; margin-right: auto;">
		<p>The benefit of these accounts is that the quotes they post are formatted well, therefore making the data cleaning process 
			minimal. We chose to delete any quote that was not tweeted by these four accounts to assure that we can 
			give the proper credit to everyone. We then focused on solely the text, removing any symbols, 
			emojis, or pictures and dropped any duplicate quotes. This left us with one column of quotes. 
		</p>  
	</div>
	
	<div class="summary">
	<h3>Machine Learning Models</h3>
		<p> To determine the best model to use we compared the results from four different models: LSTM,  Stacked LSTM, Bidirectional LSTM, and Stacked Bidirectional LSTM. 
			A description and visualization into the quality of each machine learning model will be discussed on the Visualizations page. 
		</p>

		<h5>LSTM:</h5>  
			<p>In this model, we implemented only one LSTM layer.  The LSTM uses a cell to maintain memory as well as a "carry for ensuring that the signal … is not lost as the sequence is processed" (Koehrsen).  For our LSTM model, we built the model with Embedding, LSTM, Dropout, and Dense layers.  Then, when compiling the model, we used categorical crossentropy for our loss function and adam for our optimizer.
			</p>

		<h5>Stacked LSTM:</h5>  
			<p> The difference between this model and the previous model is that we now have multiple LSTM layers stacked on top of each other.  For this model, we utilized three LSTM layers to create the stack.  In between each of the LSTM layers, there was also a dropout layer.  These dropout layers allow us to prevent overfitting in the training data (Koehrsen).  In addition, we had an Embedding and a Dense layer.  To compile the model, we used the same parameters as in the first model.  The purpose of the stacked layers is that it allows for more model complexity.
			</p>
		<h5>Bidirectional LSTM:</h5>  
			<p>For this model, only one Bidirectional LSTM layer was implemented.  In addition to this layer, we utilized an Embedding, Dropout, and Dense layer.  When compiling the model, we again used categorical crossentropy for our loss function and adam for our optimizer.  There are a few differences between an LSTM and a Bidirectional LSTM.  The Bidirectional LSTM layer ``processes sequences from both directions”(Koehrsen).  Bidirectional LSTMs involve "duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second” (Brownlee).  This idea came about because there is evidence to show that the context of a full statement can be used to interpret what is being said, rather than just the beginning of the statement (Brownlee).
			</p>
		<h5>Stacked Bidirectional LSTM:</h5>
			<p>This model is basically a combination of the third model with the second model.  We used the idea from the second model and implemented it with the Bidirectional LSTMs from the third model.  Just as done in the Stacked LSTM, for the Stacked Bidirectional LSTM, we utilized three Bidirectional LSTM layers to create the stack.  In these layers, we included dropout as well.  Additional layers in this model include an Embedding, another Dropout layer, and a Dense layer.  For this model, categorical crossentropy was again used for our loss function and adam for our optimizer.  This model allowed for the extra model complexity that comes from the Stacked LSTM logic, as well as the ability to process the sequences from both directions that come from the Bidirectional LSTM.
			</p>
			
		
		<h6> Sources: </h6>
			<p>
			J. Brownlee, “A Gentle Introduction to Long Short-Term Memory Networks by the Experts,” Machine Learning Mastery, 19-Feb-2020. [Online]. Available: https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/.
			<br/>
			W. Koehrsen, “Recurrent Neural Networks by Example in Python,” Medium, 05-Nov-2018. [Online]. Available: https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470.</p>
			</p>
	</div>

    
    
    <p id="footer"> <strong>Author:</strong> Allison Nowakowski, Amy Pitts, amd Kaitlyn Mulligan, <strong>Sources:</strong> The Web Miner, <em>Famous Quotes Database</em> </p>

</body>

</html>